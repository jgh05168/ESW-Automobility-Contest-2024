///////////////////////////////////////////////////////////////////////////////////////////////////////////
///
/// Copyright, 2021 PopcornSAR Co., Ltd. All rights reserved.
/// This software is copyright protected and proprietary to PopcornSAR Co., Ltd.
/// PopcornSAR Co., Ltd. grants to you only those rights as set out in the license conditions.
///
///////////////////////////////////////////////////////////////////////////////////////////////////////////
/// AUTOSAR VERSION                   : R20-11
/// GENERATED BY                      : PARA, Adaptive Application Generator
///////////////////////////////////////////////////////////////////////////////////////////////////////////
/// GENERATED FILE NAME               : inference.h
/// SOFTWARE COMPONENT NAME           : Inference
/// GENERATED DATE                    : 2024-11-07 14:01:17
///////////////////////////////////////////////////////////////////////////////////////////////////////////
#ifndef PARA_AA_GEN_SOFTWARE_COMPONENT_INFERENCE_AA_H
#define PARA_AA_GEN_SOFTWARE_COMPONENT_INFERENCE_AA_H
///////////////////////////////////////////////////////////////////////////////////////////////////////////
/// INCLUSION HEADER FILES
///////////////////////////////////////////////////////////////////////////////////////////////////////////
#include "inference/aa/port/fusiondata.h"
#include "inference/aa/port/inferencedata.h"
#include "inference_engine.hpp"
#include "para/swc/port_pool.h"
#include "inference/aa/image_process.hpp"
#include <vector>

namespace inference
{
    namespace aa
    {
        class InferenceBase
        {
        public:
            InferenceBase() = default;
            ~InferenceBase() = default;
            virtual bool loadModel(const char *artifactPath,
                                   std::shared_ptr<inference::aa::InferTask::Grey> imgProcess) = 0;
            /// Starts the inference task until stopped.
            // virtual void startInference() = 0;
            /// Stops the inference task if running.
        };
        namespace IntelInferenceEngine
        {
            class RLInferenceModel : public inference::aa::InferenceBase
            {
            public:
                RLInferenceModel() {}

                virtual bool loadModel(const char *artifactPath, std::shared_ptr<inference::aa::InferTask::Grey> imgProcess);
                // virtual void startInference();
                deepracer::type::InferenceDataNode sensorCB(const deepracer::service::fusiondata::proxy::events::FEvent::FEvent::SampleType &output);

            private:
                std::shared_ptr<inference::aa::InferTask::ImgProcessBase> imgProcess_;
                InferenceEngine::Core core_;
                /// Inference request object
                InferenceEngine::InferRequest inferRequest_;
                std::vector<std::unordered_map<std::string, int>> paramsArr_;
                std::vector<std::string> inputNamesArr_;
                /// Name of the output layer
                std::string outputName_;
            };

        }

        class Inference
        { 
        public:
            /// @brief Constructor
            Inference();

            /// @brief Destructor
            ~Inference();

            /// @brief Initialize software component
            bool Initialize();

            /// @brief Start software component
            void Start();

            /// @brief Terminate software component
            void Terminate();

        private:
            /// @brief Run software component
            void Run();

            /// @brief Task Receive REvent Cyclic
            void TaskReceiveFEventCyclic();

            /// @brief Task Request RMethod And RField
            void TaskRequestFMethod();

            // PPort 데이터 처리 함수
            void OnReceiveFEvent(const deepracer::service::fusiondata::proxy::events::FEvent::FEvent::SampleType &FEvent);

        private:
            /// @brief Pool of port
            ::para::swc::PortPool m_workers;

            /// @brief Logger for software component
            ara::log::Logger &m_logger;

            /// @brief Instance of Port {Inference.FusionData}
            std::shared_ptr<inference::aa::port::FusionData> m_FusionData;

            /// @brief Instance of Port {Inference.InferenceData}
            std::shared_ptr<inference::aa::port::InferenceData> m_InferenceData;

            std::shared_ptr<inference::aa::IntelInferenceEngine::RLInferenceModel> m_Model;
        };

    } /// namespace aa
} /// namespace inference

#endif /// PARA_AA_GEN_SOFTWARE_COMPONENT_INFERENCE_AA_H